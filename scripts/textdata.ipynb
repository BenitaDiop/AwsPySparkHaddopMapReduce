{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/Cables.png)\n",
    "\n",
    "# LOAD PACKAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1590662666009_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-43-223.ec2.internal:20888/proxy/application_1590662666009_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-223.ec2.internal:8042/node/containerlogs/container_1590662666009_0004_01_000001/livy\">Link</a></td><td></td></tr><tr><td>4</td><td>application_1590662666009_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-43-223.ec2.internal:20888/proxy/application_1590662666009_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-223.ec2.internal:8042/node/containerlogs/container_1590662666009_0005_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a570980c3575479a975a89eabe4c7351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1590662666009_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-43-223.ec2.internal:20888/proxy/application_1590662666009_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-223.ec2.internal:8042/node/containerlogs/container_1590662666009_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.2.1\n",
      "  Using cached matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib==3.2.1) (1.14.5)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib==3.2.1) (1.13.0)\n",
      "Installing collected packages: pyparsing, python-dateutil, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 pyparsing-2.4.7 python-dateutil-2.8.1\n",
      "\n",
      "Collecting pandas==1.0.3\n",
      "  Using cached pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1590671609804-0/lib/python3.6/site-packages (from pandas==1.0.3) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==1.0.3) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==1.0.3) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==1.0.3) (1.13.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.0.3\n",
      "\n",
      "Collecting seaborn==0.10.0\n",
      "  Using cached seaborn-0.10.0-py3-none-any.whl (215 kB)\n",
      "Collecting scipy>=1.0.1\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /mnt/tmp/1590671609804-0/lib64/python3.6/site-packages (from seaborn==0.10.0) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from seaborn==0.10.0) (1.14.5)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /mnt/tmp/1590671609804-0/lib64/python3.6/site-packages (from seaborn==0.10.0) (1.0.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /mnt/tmp/1590671609804-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn==0.10.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1590671609804-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn==0.10.0) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/tmp/1590671609804-0/lib64/python3.6/site-packages (from matplotlib>=2.1.2->seaborn==0.10.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/tmp/1590671609804-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn==0.10.0) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.22.0->seaborn==0.10.0) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn==0.10.0) (1.13.0)\n",
      "Installing collected packages: scipy, seaborn\n",
      "Successfully installed scipy-1.4.1 seaborn-0.10.0\n",
      "\n",
      "Collecting numpy==1.18.4\n",
      "  Using cached numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.14.5\n",
      "    Not uninstalling numpy at /usr/local/lib64/python3.6/site-packages, outside environment /tmp/1590671609804-0\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "Successfully installed numpy-1.18.4\n",
      "\n",
      "Processing /mnt/var/lib/livy/.cache/pip/wheels/84/30/e3/c51c5cd0229631e662d29d7b578a3e5949a4c8db033ffb70aa/pyspark-2.4.5-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.7\n",
      "  Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
      "\n",
      "Collecting ipython==7.14.0\n",
      "  Using cached ipython-7.14.0-py3-none-any.whl (782 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Using cached traitlets-4.3.3-py2.py3-none-any.whl (75 kB)\n",
      "Collecting pexpect; sys_platform != \"win32\"\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting decorator\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting pygments\n",
      "  Using cached Pygments-2.6.1-py3-none-any.whl (914 kB)\n",
      "Collecting pickleshare\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Processing /mnt/var/lib/livy/.cache/pip/wheels/b4/cb/f1/d142b3bb45d488612cf3943d8a1db090eb95e6687045ba61d1/backcall-0.1.0-py3-none-any.whl\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Using cached prompt_toolkit-3.0.5-py3-none-any.whl (351 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /mnt/tmp/1590671609804-0/lib/python3.6/site-packages (from ipython==7.14.0) (47.1.0)\n",
      "Collecting jedi>=0.10\n",
      "  Using cached jedi-0.17.0-py2.py3-none-any.whl (1.1 MB)\n",
      "Collecting ipython-genutils\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from traitlets>=4.2->ipython==7.14.0) (1.13.0)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Using cached ptyprocess-0.6.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting wcwidth\n",
      "  Using cached wcwidth-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Collecting parso>=0.7.0\n",
      "  Using cached parso-0.7.0-py2.py3-none-any.whl (100 kB)\n",
      "Installing collected packages: ipython-genutils, decorator, traitlets, ptyprocess, pexpect, pygments, pickleshare, backcall, wcwidth, prompt-toolkit, parso, jedi, ipython\n",
      "Successfully installed backcall-0.1.0 decorator-4.4.2 ipython-7.14.0 ipython-genutils-0.2.0 jedi-0.17.0 parso-0.7.0 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.5 ptyprocess-0.6.0 pygments-2.6.1 traitlets-4.3.3 wcwidth-0.1.9\n",
      "\n",
      "Processing /mnt/var/lib/livy/.cache/pip/wheels/19/20/58/cc9bc70361c517120ee83bbc4ecc35bafc5e1bfa3e9c8e93bc/subprocess.run-0.0.8-py3-none-any.whl\n",
      "Installing collected packages: subprocess.run\n",
      "Successfully installed subprocess.run-0.0.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ERROR: python36-sagemaker-pyspark 1.2.6 requires pyspark==2.3.2, which is not installed.\n",
      "\n",
      "ERROR: python36-sagemaker-pyspark 1.2.6 has requirement pyspark==2.3.2, but you'll have pyspark 2.4.5 which is incompatible."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"matplotlib==3.2.1\")\n",
    "sc.install_pypi_package(\"pandas==1.0.3\")\n",
    "sc.install_pypi_package(\"seaborn==0.10.0\")\n",
    "sc.install_pypi_package(\"numpy==1.18.4\")\n",
    "sc.install_pypi_package(\"pyspark==2.4.5\")\n",
    "sc.install_pypi_package(\"ipython==7.14.0\")\n",
    "sc.install_pypi_package(\"subprocess.run==0.0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae7ed3b9378494b87b8d6e74a438792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import rank, sum, col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ebc20a626d43b8a2623212ebbf9698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Window \n",
    "window = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/Cables.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/Cables.png)\n",
    "\n",
    "# DATA EXPLORATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611db99c3a684e09bb2e7cde44c0747a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "usr = spark.read.json(\"s3://my-little-pony/yelp/yelp_academic_dataset_user.json\")\n",
    "view = spark.read.load('s3://my-little-pony/yelp/reviews.json', format='json')\n",
    "business = spark.read.json(\"s3://my-little-pony/yelp/yelp_academic_dataset_business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7214604a54479bb459390eadf842bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|             user_id|ReviewRating|         business_id|           review_id|                text|               date|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|OwjRMXRC0KyPrIlcj...|         2.0|-MhfebM0QIsKt87iD...|xQY8N_XvtGbearJ5X...|As someone who ha...|2015-04-15 05:21:16|\n",
      "|nIJD_7ZXHq-FX8byP...|         1.0|lbrU8StCq3yDfr-QM...|UmFMZ8PyXZTY2Qcwz...|I am actually hor...|2013-12-07 03:16:52|\n",
      "|V34qejxNsCbcgD8C0...|         5.0|HQl28KMwrEKHqhFrr...|LG2ZaYiOgpr2DK_90...|I love Deagan's. ...|2015-12-05 03:18:11|\n",
      "|ofKDkJKXSKZXu5xJN...|         1.0|5JxlZaqCnk1MnbgRi...|i6g_oA9Yf9Y31qt0w...|Dismal, lukewarm,...|2011-05-27 05:30:52|\n",
      "|UgMW8bLE0QMJDCkQ1...|         4.0|IS4cv902ykd8wj1TR...|6TdNDKywdbjoTkize...|Oh happy day, fin...|2017-01-14 21:56:57|\n",
      "|5vD2kmE25YBrbayKh...|         5.0|nlxHRv1zXGT0c0K51...|L2O_INwlrRuoX05KS...|This is definitel...|2013-05-07 07:25:25|\n",
      "|aq_ZxGHiri48TUXJl...|         5.0|Pthe4qk5xh4n-ef-9...|ZayJ1zWyWgY9S_TRL...|Really good place...|2015-11-05 23:11:05|\n",
      "|dsd-KNYKMpx6ma_sR...|         5.0|FNCJpSn0tL9iqoY3J...|lpFIJYpsvDxyph-kP...|Awesome office an...|2017-07-18 18:31:54|\n",
      "|P6apihD4ASf1vpPxH...|         5.0|e_BiI4ej1CW1F0EyV...|JA-xnyHytKiOIHl_z...|Most delicious au...|2015-02-16 06:48:47|\n",
      "|jOERvhmK6_lo_XGUB...|         4.0|Ws8V970-mQt2X9CwC...|z4BCgTkfNtCu4XY5L...|I have been here ...|2009-10-13 04:16:41|\n",
      "|s5j_CRBWDCCMDJ6r7...|         5.0|PA61Rwk3AMwOEXHev...|TfVth7UNfgilv4J3e...|Maria is VERY goo...|2013-12-28 21:02:55|\n",
      "|HJECayULRM-6xh2GC...|         4.0|l-nL4BmhzpZjcavoo...|Tyx7AxYQfSRnBFUIX...|ORDER In (Deliver...|2015-10-17 01:38:13|\n",
      "|1YIQGP-a534nyksaw...|         5.0|Naa6E0YU0Wr7jCuCE...|wJMjt5C2y1RKgY0Xb...|We purchased new ...|2015-07-03 21:48:51|\n",
      "|qftVgPj_kRTildMDj...|         5.0|Ns4tjgLfqR1qawGlN...|QCxPzh7cuxJrLd6A_...|Everything that m...|2016-06-11 22:00:11|\n",
      "|5lb0POg2t-AkMFx66...|         5.0|ZlCSsWS07JulSBIQl...|qWHp2l2lysENZObh6...|Called for a 5:15...|2015-05-26 10:36:47|\n",
      "|TF4C-F5iqavACQgKT...|         1.0|7Ka9Pd8X9SRHs1D5E...|mjbs5CL4eMu4o6_Vt...|If I could give l...|2017-08-07 21:36:36|\n",
      "|2hRe26HSCAWbFRn5W...|         1.0|d4qwVw4PcN-_2mK2o...|bVTjZgRNq8Toxzvti...|10pm on a super b...|2015-02-02 06:28:00|\n",
      "|6sJN_HlM_uwpfLJ1p...|         4.0|oVuZtlCFg_zF090Nh...|Ne_2CSfcKIqXHmv_K...|A close friend wa...|2018-02-01 19:15:00|\n",
      "|kMkWON2lmw0s-M-fw...|         1.0|_iGvLfEsqDwPUxRUA...|Hy-gUXQh3RVhE8FLH...|Tried to have my ...|2017-06-28 00:39:18|\n",
      "|QodunSzok4nIYFNrT...|         3.0|poSV39UqEg-gpESXa...|UGErdm6bt48SXTVwJ...|My husband and I ...|2018-03-04 01:03:53|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "\n",
    "usr.createOrReplaceTempView(\"usr\")\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "name,\n",
    "user_id as ID,\n",
    "explode(split(elite, ',')),\n",
    "elite\n",
    "FROM usr\"\"\"\n",
    "usr1 = spark.sql(query).withColumnRenamed(\"col\", \"year\").drop(\"elite\")\n",
    "\n",
    "\n",
    "\n",
    "usr.createOrReplaceTempView(\"csv\")\n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "explode(split(elite, ',')),\n",
    "elite\n",
    "FROM csv\"\"\"\n",
    "csv =spark.sql(query)\n",
    "\n",
    "\n",
    "\n",
    "usr = usr1.groupBy(\"name\", \"ID\").count().withColumnRenamed('count', 'YRSelite').sort(\"YRSelite\", ascending=False)\n",
    "\n",
    "\n",
    "usr = usr.withColumn('eliteSTAT', \n",
    "              f.when(usr.YRSelite > 1, 1).otherwise(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t = usr.select([\"YRSelite\", \"eliteSTAT\"])\\\n",
    "            .groupBy('eliteSTAT')\\\n",
    "            .agg(f.count('YRSelite')\\\n",
    "                 .alias('Years_Num'), \n",
    "                 f.mean('YRSelite')\\\n",
    "                 .alias('Years_Avg'),\n",
    "                 f.min('YRSelite')\\\n",
    "                 .alias('Years_Min'),\n",
    "                 f.max('YRSelite')\\\n",
    "                 .alias('Years_Max'))\\\n",
    ".withColumn('total', sum(col('Years_Num')).over(window))\\\n",
    ".withColumn('Percent %', f.format_string(\"%5.0f%%\\n\", col('Years_Num')*100/col('total')))\n",
    "tab = t.drop('Percent %').withColumn('Percent', col('Years_Num')*100/col('total'))\n",
    "t.drop(col('total'))\n",
    "\n",
    "\n",
    "tab = tab.toPandas()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sub = csv.select(\"user_id\", \n",
    "                 \"fans\", \n",
    "                 \"cool\", \n",
    "                 \"useful\", \n",
    "                 \"average_stars\",\n",
    "                \"compliment_cool\",\n",
    "                \"compliment_funny\", \n",
    "                \"compliment_hot\",\n",
    "                \"compliment_more\",\n",
    "                \"compliment_note\",\n",
    "                \"compliment_plain\", \n",
    "                \"compliment_photos\",\n",
    "                \"compliment_profile\")\n",
    "usr.createOrReplaceTempView(\"us\")\n",
    "us = usr.join(sub, usr.ID == sub.user_id, \"full\")\n",
    "us = us.withColumnRenamed('compliment_cool', 'cc')\\\n",
    "       .withColumnRenamed('compliment_funny', 'cf')\\\n",
    "       .withColumnRenamed('compliment_profile', 'cpr')\\\n",
    "       .withColumnRenamed('compliment_note', 'cn')\\\n",
    "       .withColumnRenamed('compliment_more', 'cm')\\\n",
    "       .withColumnRenamed('compliment_photos', 'cph')\\\n",
    "       .withColumnRenamed('compliment_plain', 'cpl')\\\n",
    "       .withColumnRenamed('compliment_hot', 'ch').drop(\"user_id\")\n",
    "\n",
    "us.columns\n",
    "\n",
    "\n",
    "\n",
    "view.createOrReplaceTempView(\"tmp2\")\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "user_id,\n",
    "stars as ReviewRating,\n",
    "business_id,\n",
    "review_id,\n",
    "text, \n",
    "date \n",
    "FROM tmp2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "tmp2 = spark.sql(query)\n",
    "tmp2.show()\n",
    "expo = tmp2.join(us, us.ID == tmp2.user_id, \"full\")\n",
    "\n",
    "\n",
    "expo.createOrReplaceTempView(\"df\")\n",
    "query = \"\"\"\n",
    "(SELECT * FROM df)\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "df = df.distinct()\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/Cables.png)\n",
    "\n",
    "# 80/20 SPLIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a60deb4833d46d991f91fd9b5fdd602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df,test_df=df.randomSplit([0.9,0.1])\n",
    "val, tdf = test_df.randomSplit([0.99, .01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f4f06b14ec4274a6d8ff6d2214eef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Rows in FULL Data 8021122 \n",
      "\n",
      "\n",
      "Number of Rows in Test Data 7219284 \n",
      " \n",
      "\n",
      "Number of rows in Sub Test Data 7887"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Number of Rows in FULL Data {} \\n\n",
    "\n",
    "Number of Rows in Test Data {} \\n \n",
    "\n",
    "Number of rows in Sub Test Data {} \n",
    "\"\"\".format(df.count(), train_df.count(), tdf.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcbd908fcc1436c8b6d8e2dcc9de44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ea43d8fcd34e8ba65a1446958f2dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdf = tdf.toPandas()\n",
    "tdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/BenitaDiop/students/master/bin/Cables.png)\n",
    "\n",
    "# REVIEW TEXT DATA ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenization=Tokenizer(inputCol='text',outputCol='tokens')\n",
    "\n",
    "tokenized_df=tokenization.transform(expo)\n",
    "\n",
    "tokenized_df.select(\"text\", \"tokens\").show(5, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "\n",
    "refined_df=stopword_removal.transform(tokenized_df)\n",
    "\n",
    "refined_df.select(['user_id','tokens','refined_tokens']).show(2, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "\n",
    "cv_df=count_vec.fit(refined_df).transform(refined_df)\n",
    "\n",
    "cv_df.select(['user_id','refined_tokens','features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.fit(refined_df).vocabulary\n",
    "\n",
    "from pyspark.ml.feature import HashingTF,IDF\n",
    "\n",
    "hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features')\n",
    "\n",
    "hashing_df=hashing_vec.transform(refined_df)\n",
    "\n",
    "hashing_df.select(['user_id','refined_tokens','tf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec=IDF(inputCol='tf_features',outputCol='tf_idf_features')\n",
    "\n",
    "tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "\n",
    "tf_idf_df.select(['user_id','tf_idf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=spark.read.csv('Movie_reviews.csv',inferSchema=True,header=True,sep=',')\n",
    "text_df.printSchema()\n",
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "text_df.orderBy(rand()).show(10,False)\n",
    "text_df=text_df.filter(((text_df.Sentiment =='1') | (text_df.Sentiment =='0')))\n",
    "\n",
    "text_df.count()\n",
    "text_df.groupBy('Sentiment').count().show()\n",
    "\n",
    "\n",
    "text_df.printSchema()\n",
    "text_df = text_df.withColumn(\"Label\", text_df.Sentiment.cast('float')).drop('Sentiment')\n",
    "text_df.orderBy(rand()).show(10,False)\n",
    "text_df.groupBy('label').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add length to the dataframe\n",
    "from pyspark.sql.functions import length\n",
    "text_df=text_df.withColumn('length',length(text_df['Review']))\n",
    "\n",
    "text_df.orderBy(rand()).show(10,False)\n",
    "text_df.groupBy('Label').agg({'Length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "tokenization=Tokenizer(inputCol='Review',outputCol='tokens')\n",
    "tokenized_df=tokenization.transform(text_df)\n",
    "\n",
    "tokenized_df.show()\n",
    "\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "refined_text_df=stopword_removal.transform(tokenized_df)\n",
    "\n",
    "refined_text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "refined_text_df = refined_text_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n",
    "\n",
    "refined_text_df.orderBy(rand()).show(10)\n",
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "cv_text_df=count_vec.fit(refined_text_df).transform(refined_text_df)\n",
    "cv_text_df.select(['refined_tokens','token_count','features','Label']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data for building model\n",
    "model_text_df=cv_text_df.select(['features','token_count','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_assembler = VectorAssembler(inputCols=['features','token_count'],outputCol='features_vec')\n",
    "model_text_df = df_assembler.transform(model_text_df)\n",
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "#split the data \n",
    "training_df,test_df=model_text_df.randomSplit([0.75,0.25])\n",
    "training_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupBy('Label').count().show()\n",
    "log_reg=LogisticRegression(featuresCol='features_vec',labelCol='Label').fit(training_df)\n",
    "results=log_reg.evaluate(test_df).predictions\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "true_postives = results[(results.Label == 1) & (results.prediction == 1)].count()\n",
    "true_negatives = results[(results.Label == 0) & (results.prediction == 0)].count()\n",
    "false_positives = results[(results.Label == 0) & (results.prediction == 1)].count()\n",
    "false_negatives = results[(results.Label == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "print(recall)\n",
    "\n",
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "print(precision)\n",
    "\n",
    "\n",
    "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
